{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> The heart of the library\n",
    "\n",
    "This component defines how to generaly evaluate the `Learner`s used by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from fastai.vision.learner import Learner\n",
    "from fastai.data.block import DataBlock\n",
    "from fastai.data.load import DataLoader\n",
    "from fastai.data.transforms import IndexSplitter\n",
    "\n",
    "from typing import Callable, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from fastai.vision.learner import unet_learner\n",
    "from fastai.data.block import DataBlock\n",
    "from fastai.data.load import DataLoader\n",
    "from fastai.data.transforms import IndexSplitter\n",
    "\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "def evaluate(\n",
    "    datablock_hparams: dict, # The hyperparameters used to get and load the data.\n",
    "    dataloader_hparams: dict, # The hyperparameters used to define how the data is supplied to the learner.\n",
    "    technique: BaseCrossValidator, # The technique used to split the data.\n",
    "    learner_hparams: dict,  # The parameters used to build the learner (backbone, cbs...). Those hyperparams are used to build all the models.\n",
    "    learning_hparams: dict, # The parameters used to train the learner (learning_rate, freeze_epochs).\n",
    "    learning_mode: str = \"finetune\", # The learning mode: random or finetune.\n",
    "    saving_hparams: dict = {\"save_best\": False}, # The model saving information {save_best: bool, metric: str}.\n",
    "    verbose: bool = False # Whether the method shows logging messages\n",
    "):\n",
    "    \n",
    "    # defines all the metrics used in the training and evaluation phases\n",
    "    metrics = [\"validation\"]\n",
    "    other_metrics = learner_hparams[\"metrics\"] if \"metrics\" in learner_hparams else []\n",
    "    all_metrics = list(map(lambda metric: metric if type(metric) == str else metric.__class__.__name__, metrics + other_metrics))\n",
    "    results = dict([[metric, []] for metric in all_metrics])\n",
    "    \n",
    "    # gets all the data\n",
    "    get_items_form = \"get_items\" if \"get_items\" in datablock_hparams else \"get_x\"\n",
    "    get_items = [datablock_hparams[get_items_form], datablock_hparams[\"get_y\"]]\n",
    "\n",
    "    X = get_items[0](dataloader_hparams[\"source\"])\n",
    "    y = [get_items[1](x) for x in X]\n",
    "    best_score = 0\n",
    "    index = 1\n",
    "    if verbose: print(\"Starting the training for a new model\")\n",
    "    for _, testing_indexes in technique.split(X, y):\n",
    "        if verbose: print(f\"Training the fold {index}\"); index += 1\n",
    "        dls = DataBlock(**datablock_hparams).dataloaders(**dataloader_hparams)\n",
    "        learner = unet_learner(dls, **learner_hparams).to_fp16()\n",
    "        if learning_mode == \"random\":\n",
    "            learner.fit_one_cycle(**learning_hparams)\n",
    "        elif learning_mode == \"finetune\":\n",
    "            learner.fine_tune(**learning_hparams)\n",
    "        else:\n",
    "            raise Exception(f\"{learning_mode} is not a learning_mode. Use 'random' or 'finetune' instead.\")\n",
    "        \n",
    "        # replaces the training dls and tests the model\n",
    "        datablock_hparams[\"splitter\"] = IndexSplitter(testing_indexes)\n",
    "        learner.dls = DataBlock(**datablock_hparams).dataloaders(**dataloader_hparams)\n",
    "        log = \"\"\n",
    "        for metric, metric_value in zip(results, learner.validate()):\n",
    "            results[metric] += [metric_value]\n",
    "            log += f\"  {metric}: {metric_value}  /\"\n",
    "        \n",
    "        if verbose: print(f\"Test results for the model. {log[:-1]}\")\n",
    "        \n",
    "        # saves the model\n",
    "        if saving_hparams[\"save_best\"]:\n",
    "            score = results[saving_hparams[\"metric\"]][-1]\n",
    "            if score > best_score:\n",
    "                if verbose: print(f\"Saving best model because {saving_hparams['metric']} {round(score, 4)} >= {round(best_score, 4)}.\")\n",
    "                best_score = score\n",
    "                learner.save(saving_hparams[\"model_name\"])\n",
    "        \n",
    "        # wipes the memory of the gpu\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluation",
   "language": "python",
   "name": "evaluation"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
