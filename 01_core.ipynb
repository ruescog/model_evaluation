{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> The heart of the library\n",
    "\n",
    "This component defines how to generaly evaluate the `Learner`s used by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from fastai.vision.learner import Learner\n",
    "from fastai.data.block import DataBlock\n",
    "from fastai.data.load import DataLoader\n",
    "from fastai.data.transforms import IndexSplitter\n",
    "\n",
    "from typing import Callable, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from fastai.vision.learner import unet_learner\n",
    "from fastai.data.block import DataBlock\n",
    "from fastai.data.load import DataLoader\n",
    "from fastai.data.transforms import IndexSplitter\n",
    "\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "def evaluate(\n",
    "    datablock_hparams: dict, # The hyperparameters used to get and load the data.\n",
    "    dataloader_hparams: dict, # The hyperparameters used to define how the data is supplied to the learner.\n",
    "    technique: BaseCrossValidator, # The technique used to split the data.\n",
    "    learner_hparams: dict,  # The parameters used to build the learner (backbone, cbs...). Those hyperparams are used to build all the models.\n",
    "    learning_hparams: dict, # The parameters used to train the learner (learning_rate, freeze_epochs)\n",
    "    learning_mode: str = \"finetune\" # The learning mode: random or finetune.\n",
    "):\n",
    "    \n",
    "    # Defines all the metrics used in the training and evaluation phases\n",
    "    metrics = [\"validation\"]\n",
    "    other_metrics = learner_hparams[\"metrics\"] if \"metrics\" in learner_hparams else []\n",
    "    all_metrics = list(map(lambda metric: metric if type(metric) == str else metric.__class__.__name__, metrics + other_metrics))\n",
    "    results = dict([[metric, []] for metric in all_metrics])\n",
    "    \n",
    "    # Gets all the data\n",
    "    get_items_form = \"get_items\" if \"get_items\" in datablock_hparams else \"get_x\"\n",
    "    get_items = [datablock_hparams[get_items_form], datablock_hparams[\"get_y\"]]\n",
    "\n",
    "    X = get_items[0](dataloader_hparams[\"source\"])\n",
    "    y = [get_items[1](x) for x in X]\n",
    "    for _, testing_indexes in technique.split(X, y):\n",
    "        dls = DataBlock(**datablock_hparams).dataloaders(**dataloader_hparams)\n",
    "        learner = unet_learner(dls, **learner_hparams).to_fp16()\n",
    "        if learning_mode == \"random\":\n",
    "            learner.fit_one_cycle(**learning_hparams)\n",
    "        elif learning_mode == \"finetune\":\n",
    "            learner.fine_tune(**learning_hparams)\n",
    "        else:\n",
    "            raise Exception(f\"{learning_mode} is not a learning_mode. Use 'random' or 'finetune' instead.\")\n",
    "        \n",
    "        # Replaces the training dls and tests the model\n",
    "        datablock_hparams[\"splitter\"] = IndexSplitter(testing_indexes)\n",
    "        learner.dls = DataBlock(**datablock_hparams).dataloaders(**dataloader_hparams)\n",
    "        for metric, metric_value in zip(results, learner.validate()):\n",
    "            results[metric] += [metric_value]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
